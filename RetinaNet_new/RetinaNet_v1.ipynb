{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfbcce",
   "metadata": {},
   "source": [
    "# RetinaNet for Object Detection\n",
    "\n",
    "This notebook documents the training done on `RetinaNet` for datasets sourced by cerv.AI team. The said datasets are as follows:\n",
    "1. https://universe.roboflow.com/cervitester-colposcopy-yihp4/colposcopy\n",
    "2. https://universe.roboflow.com/cervitester-colposcopy-yihp4/acetic_acid\n",
    "3. https://universe.roboflow.com/madhura/merged-acetic-acid/dataset/3\n",
    "\n",
    "## Additional Notes\n",
    "1. Due to the balanced nature of the datasets, this training provides **no data augmentations**. It is only when the training with the other datasets that augmentations will be applied.\n",
    "    - However, RetinaNet is excellent for imbalanced datasets due to the **Focal Loss** function.\n",
    "\n",
    "2. The datasets are all from **IARC Cervical Image Cancer Bank**\n",
    "\n",
    "## References \n",
    "1. [Fine-Tuning an Object Detection Model](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "2. [Focal Loss](https://paperswithcode.com/method/focal-loss)\n",
    "3. [RetinaNet (Theory)](https://paperswithcode.com/method/retinanet)\n",
    "4. [RetinaNet (Pytorch implementation)](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html)\n",
    "5. [Paper on Focal Loss](https://arxiv.org/abs/1708.02002)\n",
    "6. [Blog 1 on RetinaNet](https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d)\n",
    "7. [Blog 2 on RetinaNet](https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/)\n",
    "8. [Blog 3 on RetinaNet](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4)\n",
    "9. [Blog 4 on RetinaNet](https://analyticsindiamag.com/what-is-retinanet-ssd-focal-loss/)\n",
    "10. [Blog 5 on RetinaNet](https://towardsdatascience.com/object-detection-on-aerial-imagery-using-retinanet-626130ba2203)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa194c81",
   "metadata": {},
   "source": [
    "## Dataset Class and DataLoading\n",
    "\n",
    "The dataset class is based on the typical COCO format. Instead of storing the image on dedicated arrays, which results in large space complexity, the dataset instead accesses the image on the `load_image` method through the root folder, set name (either as train or test), and the image IDs.\n",
    "\n",
    "The dataloader stores the methods for each images. For each iteration of the pipline, the dataloader appleis the necessary methods to each dataset itself. You can refer to the training pipeline to see where this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import collections\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import skimage\n",
    "\n",
    "#* Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#* Dataset class\n",
    "class CocoDataset(Dataset):\n",
    "    \"\"\"Coco dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, set_name='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): COCO directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.set_name = set_name\n",
    "        self.transform = transform\n",
    "\n",
    "        self.coco      = COCO(os.path.join(self.root_dir, self.set_name,'_annotations.coco.json'))\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "\n",
    "        self.load_classes()\n",
    "\n",
    "    def load_classes(self):\n",
    "        # load class names (name -> label)\n",
    "        categories = self.coco.loadCats(self.coco.getCatIds())\n",
    "        categories.sort(key=lambda x: x['id'])\n",
    "\n",
    "        self.classes             = {}\n",
    "        self.coco_labels         = {}\n",
    "        self.coco_labels_inverse = {}\n",
    "        for c in categories:\n",
    "            self.coco_labels[len(self.classes)] = c['id']\n",
    "            self.coco_labels_inverse[c['id']] = len(self.classes)\n",
    "            self.classes[c['name']] = len(self.classes)\n",
    "\n",
    "        # also load the reverse (label -> name)\n",
    "        self.labels = {}\n",
    "        for key, value in self.classes.items():\n",
    "            self.labels[value] = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.load_image(idx)\n",
    "        annot = self.load_annotations(idx)\n",
    "        sample = {'img': img, 'annot': annot}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        path       = os.path.join(self.root_dir,self.set_name, image_info['file_name'])\n",
    "        img = skimage.io.imread(path)\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = skimage.color.gray2rgb(img)\n",
    "\n",
    "        return img.astype(np.float32)/255.0\n",
    "\n",
    "    def load_annotations(self, image_index):\n",
    "        # get ground truth annotations\n",
    "        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n",
    "        annotations     = np.zeros((0, 5))\n",
    "\n",
    "        # Catch function in case an image has no annotations\n",
    "        if len(annotations_ids) == 0:\n",
    "            return annotations\n",
    "\n",
    "        # parse annotations\n",
    "        coco_annotations = self.coco.loadAnns(annotations_ids)\n",
    "        for idx, a in enumerate(coco_annotations):\n",
    "\n",
    "            # some annotations have basically no width / height, skip them\n",
    "            if a['bbox'][2] < 1 or a['bbox'][3] < 1:\n",
    "                continue\n",
    "\n",
    "            annotation        = np.zeros((1, 5))\n",
    "            annotation[0, :4] = a['bbox']\n",
    "            annotation[0, 4]  = self.coco_label_to_label(a['category_id'])\n",
    "            annotations       = np.append(annotations, annotation, axis=0)\n",
    "\n",
    "        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n",
    "        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def coco_label_to_label(self, coco_label):\n",
    "        return self.coco_labels_inverse[coco_label]\n",
    "\n",
    "\n",
    "    def label_to_coco_label(self, label):\n",
    "        return self.coco_labels[label]\n",
    "\n",
    "    def image_aspect_ratio(self, image_index):\n",
    "        image = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        return float(image['width']) / float(image['height'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401bc1a",
   "metadata": {},
   "source": [
    "## Loading of Data to DataLoader\n",
    "\n",
    "The `DataLoader` contains a `collater` and `sampler`. \n",
    "\n",
    "- The `collater` merges a list of samples to form a mini-batch of Tensor(s), which is useful for batched loading.\n",
    "    - In simpler (but not exact) terms, it returns the images, annotations, and additional padding if defined into tensors\n",
    "    - These can be therefore, easily learned by the model and parallelized using batched nodes\n",
    "- The `sampler` defines the strategy to draw samples from the dataset. If specified, shuffle must not be specified.\n",
    "    - In some cases, ussing `shuffle` is simpler, but a dedicated `AspectRatioBasedSampler` function is contained\n",
    "    - It simply uses the `random` library's shuffle function. In reality, the simple `shuffle=True` also works.\n",
    "\n",
    "- Use the `transforms` library of torch to apply your own transformations in `data_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf86d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 dataloader workers every process\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "device = \"cuda\"\n",
    "root_dirs = ['datasets/merged']\n",
    "\n",
    "#* Custom collater for the dataloader\n",
    "def collater(data):\n",
    "\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [s['annot'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "        \n",
    "    widths = [int(s.shape[0]) for s in imgs]\n",
    "    heights = [int(s.shape[1]) for s in imgs]\n",
    "    batch_size = len(imgs)\n",
    "\n",
    "    max_width = np.array(widths).max()\n",
    "    max_height = np.array(heights).max()\n",
    "\n",
    "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = imgs[i]\n",
    "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "    \n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                #print(annot.shape)\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "\n",
    "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': padded_imgs, 'annot': annot_padded, 'scale': scales}\n",
    "\n",
    "#* sampling method\n",
    "class AspectRatioBasedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size, drop_last):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = self.group_images()\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.groups)\n",
    "        for group in self.groups:\n",
    "            yield group\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.data_source) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def group_images(self):\n",
    "        # determine the order of the images\n",
    "        order = list(range(len(self.data_source)))\n",
    "        order.sort(key=lambda x: self.data_source.image_aspect_ratio(x))\n",
    "\n",
    "        # divide into groups, one group = one batch\n",
    "        return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]\n",
    "\n",
    "#* Data Transformation (You can define your own here)\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"test\": transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "dataset_train = CocoDataset(root_dir=\"/datasets\",set_name=\"train\",transform=data_transform['train']) \n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False)\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4585a9",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "\n",
    "### Focal Loss\n",
    "Formally, it is given by the following:\n",
    "$$FL_{p_t} = -(1-p)^{\\gamma}\\log(p_t)$$\n",
    "where $\\gamma$ is a tunable parameter. It basically adds a loss factor $-(1-p)^{\\gamma}$ to the standard cross entropy criterion. \n",
    "- Setting $\\gamma > 0$ reduces the relative loss for well-classified examples ($p_t > 5), emphasizing the hard, misclassified examples.\n",
    "\n",
    "- There is also [this repo](https://github.com/itakurah/Focal-loss-PyTorch) that implements it for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "# from torchvision.models import ResNet50_Weights\n",
    "from torchinfo import summary\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "#* Define the Focal Loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the Focal Loss criterion. See the documentation in the notebook.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2, alpha = 0.25, size_average = True): #For default values: https://pytorch.org/vision/main/generated/torchvision.ops.sigmoid_focal_loss.html\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha]) # store the p and 1-p\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "    \n",
    "    def forward(self,input,target):\n",
    "        if input.dim() > 2:\n",
    "            # Applying transmutations to data\n",
    "            input = input.view(input.size(0), input.size(1),-1) # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2) #N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2)) # N,H*W, C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "        \n",
    "        # Logits calculation for probability\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1) # Convert to row vec\n",
    "        pt = logpt.data.exp()\n",
    "\n",
    "        # if alpha is present\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data) #ensures the alpha is the same data type as input (Tensor)\n",
    "            at = self.alpha.gather()\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedc3edf",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### Coco Metrics\n",
    "- The source code for this implementation is found [here](https://github.com/nightrome/cocostuffapi/blob/master/PythonAPI/pycocotools/cocoeval.py). \n",
    "\n",
    "- The high-level understanding is that it returns the **Mean Average Precision** (MaP) for various sizes (Small, Medium, Large) and for different percentage (50%, 75%, range of 50-95%).\n",
    "\n",
    "- The calculation of MaP is done through Intersection Over Union (IoU), which simply calculates the toal intersection of the prediction with the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "def calc_iou(a, b):\n",
    "    \"\"\"\n",
    "    Defines the Intersection over Union (IoU) metric which utilizes the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU\n",
    "def evaluate_coco(dataset, model, threshold=0.05):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # start collecting results\n",
    "        results = []\n",
    "        image_ids = []\n",
    "\n",
    "        for index in range(len(dataset)):\n",
    "            image, target, scale = dataset[index]\n",
    "\n",
    "            # run network\n",
    "            if torch.cuda.is_available():\n",
    "                scores, labels, boxes = model(image.cuda().float().unsqueeze(0))\n",
    "            else:\n",
    "                scores, labels, boxes = model(image.float().unsqueeze(0))\n",
    "            scores = scores.cpu()\n",
    "            labels = labels.cpu()\n",
    "            boxes  = boxes.cpu()\n",
    "\n",
    "            # correct boxes for image scale\n",
    "            boxes /= scale\n",
    "\n",
    "            if boxes.shape[0] > 0:\n",
    "                # change to (x, y, w, h) (MS COCO standard)\n",
    "                boxes[:, 2] -= boxes[:, 0]\n",
    "                boxes[:, 3] -= boxes[:, 1]\n",
    "\n",
    "                # compute predicted labels and scores\n",
    "                #for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "                for box_id in range(boxes.shape[0]):\n",
    "                    score = float(scores[box_id])\n",
    "                    label = int(labels[box_id])\n",
    "                    box = boxes[box_id, :]\n",
    "\n",
    "                    # scores are sorted, so we can break\n",
    "                    if score < threshold:\n",
    "                        break\n",
    "\n",
    "                    # append detection for each positively labeled class\n",
    "                    image_result = {\n",
    "                        'image_id'    : dataset.image_ids[index],\n",
    "                        'category_id' : dataset.label_to_coco_label(label),\n",
    "                        'score'       : float(score),\n",
    "                        'bbox'        : box.tolist(),\n",
    "                    }\n",
    "\n",
    "                    # append detection to results\n",
    "                    results.append(image_result)\n",
    "\n",
    "            # append image to list of processed images\n",
    "            image_ids.append(dataset.image_ids[index])\n",
    "\n",
    "            # print progress\n",
    "            print('{}/{}'.format(index, len(dataset)), end='\\r')\n",
    "\n",
    "        if not len(results):\n",
    "            return\n",
    "\n",
    "        # write output\n",
    "        json.dump(results, open('{}_bbox_results.json'.format(dataset.set_name), 'w'), indent=4)\n",
    "\n",
    "        # load results in COCO evaluation tool\n",
    "        coco_true = dataset.coco\n",
    "        coco_pred = coco_true.loadRes('{}_bbox_results.json'.format(dataset.set_name))\n",
    "\n",
    "        # run COCO evaluation\n",
    "        coco_eval = COCOeval(coco_true, coco_pred, 'bbox')\n",
    "        coco_eval.params.imgIds = image_ids\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        model.train()\n",
    "        # Optional: extract mAP@0.5 (index 1) and mAP@0.5:0.95 (index 0)\n",
    "        map_50 = coco_eval.stats[1]\n",
    "        map_5095 = coco_eval.stats[0]\n",
    "        print(f\"mAP@0.5: {map_50:.4f}, mAP@0.5:0.95: {map_5095:.4f}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d007794",
   "metadata": {},
   "source": [
    "# Utility Classes \n",
    "\n",
    "These classes are **operations** that the model requires. Of course, some of the methods here are already available in Pytorch. Keep in mind that the original programming of this model occured before Pytorch 2.0, hence some premature definition.\n",
    "\n",
    "The second reason is for modularity since these are run as separate scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BBoxTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, mean=None, std=None):\n",
    "        super(BBoxTransform, self).__init__()\n",
    "        if mean is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n",
    "            else:\n",
    "                self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))\n",
    "\n",
    "        else:\n",
    "            self.mean = mean\n",
    "        if std is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n",
    "            else:\n",
    "                self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n",
    "        else:\n",
    "            self.std = std\n",
    "\n",
    "    def forward(self, boxes, deltas):\n",
    "\n",
    "        widths  = boxes[:, :, 2] - boxes[:, :, 0]\n",
    "        heights = boxes[:, :, 3] - boxes[:, :, 1]\n",
    "        ctr_x   = boxes[:, :, 0] + 0.5 * widths\n",
    "        ctr_y   = boxes[:, :, 1] + 0.5 * heights\n",
    "\n",
    "        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n",
    "        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n",
    "        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n",
    "        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n",
    "\n",
    "        pred_ctr_x = ctr_x + dx * widths\n",
    "        pred_ctr_y = ctr_y + dy * heights\n",
    "        pred_w     = torch.exp(dw) * widths\n",
    "        pred_h     = torch.exp(dh) * heights\n",
    "\n",
    "        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n",
    "        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n",
    "        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n",
    "        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "class ClipBoxes(nn.Module):\n",
    "\n",
    "    def __init__(self, width=None, height=None):\n",
    "        super(ClipBoxes, self).__init__()\n",
    "\n",
    "    def forward(self, boxes, img):\n",
    "\n",
    "        batch_size, num_channels, height, width = img.shape\n",
    "\n",
    "        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n",
    "        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n",
    "\n",
    "        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n",
    "        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n",
    "      \n",
    "        return boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecae2a",
   "metadata": {},
   "source": [
    "## **RetinaNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9b97c",
   "metadata": {},
   "source": [
    "See the documentation [here](https://paperswithcode.com/method/retinanet-rs). It also has a medium article [here](https://freedium.cfd/https://medium.com/@evertongomede/retinanet-advancing-object-detection-in-computer-vision-719ceb744308).\n",
    "\n",
    "- For the classification head, a separate model is used as a backbone. To use more efficient resources, we will be utilizing the smaller model **ResNet50** with 50 neural layers. However, all ResNet models are contained in the model definition, so you can use the other layers avaialable. There is also a help entry for the available layers (18,50,101,152)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abdf03",
   "metadata": {},
   "source": [
    "### **Some Important Model Notes** (see article [here](https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcad6e8",
   "metadata": {},
   "source": [
    "- **Anchor boxes** are used to generate the region proposals. Previously, selective search and edge boxes weere used. However, this was impossible to generate with standard convolutions that are utilized by most CNNs.\n",
    "\n",
    "- Each representative region (in the article, it is a 50x50 pixel) is fed to a **regression head** and a **classification head**. Usually used is the ResNet models.\n",
    "- However, the feature map created after multiple subsampling loses a lot of semantic information at low level, thus unable to detet small objects in images. To solve this, the model uses **Feature Pyramid Networks**\n",
    "\n",
    "#### **[Feature Pyramid Networks](https://arxiv.org/abs/1612.03144)**\n",
    "- Though convnets are robust to variance in scale, all the top entries in ImageNet or COCO have used multi-scale testing on featurized image pyramids.\n",
    "\n",
    "- We have to take images at different sizes say 256 x 256, 300 x 300, 500 x 500 and 800 x 800 etc, calculate feature maps for each of this image and then apply non-maxima supression over all these detected positive anchor boxes. This is a very costly operation and inference times gets high.\n",
    "\n",
    "- The authors of this paper observed that deep convnet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. For example, take a Resnet architecture and instead of just using the final feature map as shown in RPN network, take feature maps before every pooling (subsampling) layer. \n",
    "\n",
    "- Perform the same operations as for **Region Proposal Network** RPN on each of these feature maps and finally combine them using non-maxima supression. This is the crude way of building the feature pyramid networks. \n",
    "\n",
    "- But there are large semantic gaps caused by different depths. The high resolution maps (earlier layers) have low-level features that harm their representational capacity for object detection. To achieve this goal, the authors relayed on a architecture that combines low-resolution, semantically strong features with high-resolution, semantically strong features via top-down pathway and lateral connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00f774",
   "metadata": {},
   "source": [
    "### **[Non-Max Suppression](https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792964b3",
   "metadata": {},
   "source": [
    "- Non-maximum suppression (NMS) is a post-processing technique used in object detection to eliminate duplicate detections and select the most relevant detected objects.  This helps reduce false positives and the computational complexity of a detection algorithm.\n",
    "\n",
    "It's step by step process are as follows:\n",
    "\n",
    "1. **Bounding Box Generation** - The object detection model generates multiple bounding boxes for objects in an image each with a confidence score indicating the likelihood of the presence of an object.\n",
    "\n",
    "2. **Sort By Confidence Score** - All the generated bounding boxes are sorted in descending order based on their confidence scores.\n",
    "\n",
    "3. Suppression Process\n",
    "    1. **Select the Box with the Highest Confidence**: The box with the highest confidence score is considered a final detection.\n",
    "\n",
    "    2. **Calculate Intersection over Union (IoU)**: IoU is a measure of the overlap between two bounding boxes.\n",
    "\n",
    "    3. **Suppress Overlapping Boxes**: If the IoU of any remaining boxes with the selected box exceeds a threshold (usually 0.5) they are suppressed and discarded.\n",
    "    \n",
    "    4. **Repeat the Process**: This continues until all boxes have been either selected as a detection or suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcf6d4",
   "metadata": {},
   "source": [
    "## **Anchors Generation**\n",
    "\n",
    "Object detection models utilize anchor boxes to make bounding box predictions. In order to predict and localize many different objects in an image, most state of the art object detection models such as EfficientDet and the YOLO models start with anchor boxes as a prior, and adjust from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        # compute anchors over all pyramid levels\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32))\n",
    "\n",
    "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales w.r.t. a reference window.\n",
    "    \"\"\"\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = np.array([0.5, 1, 2])\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    num_anchors = len(ratios) * len(scales)\n",
    "\n",
    "    # initialize output anchors\n",
    "    anchors = np.zeros((num_anchors, 4))\n",
    "\n",
    "    # scale base_size\n",
    "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "    # compute areas of anchors\n",
    "    areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "    # correct for ratios\n",
    "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def compute_shape(image_shape, pyramid_levels):\n",
    "    \"\"\"Compute shapes based on pyramid levels.\n",
    "\n",
    "    :param image_shape:\n",
    "    :param pyramid_levels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image_shape = np.array(image_shape[:2])\n",
    "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
    "    return image_shapes\n",
    "\n",
    "def anchors_for_shape(\n",
    "    image_shape,\n",
    "    pyramid_levels=None,\n",
    "    ratios=None,\n",
    "    scales=None,\n",
    "    strides=None,\n",
    "    sizes=None,\n",
    "    shapes_callback=None,\n",
    "):\n",
    "\n",
    "    image_shapes = compute_shape(image_shape, pyramid_levels)\n",
    "\n",
    "    # compute anchors over all pyramid levels\n",
    "    all_anchors = np.zeros((0, 4))\n",
    "    for idx, p in enumerate(pyramid_levels):\n",
    "        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
    "        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
    "        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "def shift(shape, stride, anchors):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    A = anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "    all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "    return all_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13dbf1",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms # Non-max suppression\n",
    "\n",
    "#* Store the urls of all ResNet models to easily swap to other models\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "#* Define the RetinaNet Model architecture: Regression Head, Pyramid Networks, Classification Backbone, etc.\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "\n",
    "        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        # out is Batch_size x Channels x Width x Height, with C = 4*num_anchors\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        # After self.output, the shape is [B, C, W, H].\n",
    "        # The model permutes it to [B, W, H, C] so that:\n",
    "        # For every pixel location on the feature map (W x H), it has all anchor predictions grouped together.\n",
    "        # Easier to reshape into a flat list of bounding box predictions.\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 4)\n",
    "        \n",
    "        #After permute: shape = [B, W, H, num_anchors * 4]\n",
    "        #.view(B, -1, 4) collapses spatial dimensions and anchors into a single list of box predictions:\n",
    "        #Output shape: [B,W×H×num_anchors,4]\n",
    "        #Output shape: [B,W×H×num_anchors,4]\n",
    "\n",
    "        #So you're getting a 3D tensor where:\n",
    "        #- B is batch size.\n",
    "        #- Second dim is the total number of anchor boxes.\n",
    "        #- Last dim is the 4 regression values per anchor.\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes + n_anchors\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        if block == BasicBlock:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv2.out_channels, self.layer3[layers[2] - 1].conv2.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv2.out_channels]\n",
    "        elif block == Bottleneck:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv3.out_channels, self.layer3[layers[2] - 1].conv3.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv3.out_channels]\n",
    "        else:\n",
    "            raise ValueError(f\"Block type {block} not understood\")\n",
    "\n",
    "        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2])\n",
    "\n",
    "        self.regressionModel = RegressionModel(256)\n",
    "        self.classificationModel = ClassificationModel(256, num_classes=num_classes)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "\n",
    "        self.focalLoss = FocalLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        prior = 0.01\n",
    "\n",
    "        self.classificationModel.output.weight.data.fill_(0)\n",
    "        self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
    "\n",
    "        self.regressionModel.output.weight.data.fill_(0)\n",
    "        self.regressionModel.output.bias.data.fill_(0)\n",
    "\n",
    "        self.freeze_bn()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if self.training:\n",
    "            img_batch, annotations = inputs\n",
    "        else:\n",
    "            img_batch = inputs\n",
    "\n",
    "        x = self.conv1(img_batch)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        features = self.fpn([x2, x3, x4])\n",
    "\n",
    "        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        anchors = self.anchors(img_batch)\n",
    "\n",
    "        if self.training:\n",
    "            return self.focalLoss(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
    "\n",
    "            finalResult = [[], [], []]\n",
    "\n",
    "            finalScores = torch.Tensor([])\n",
    "            finalAnchorBoxesIndexes = torch.Tensor([]).long()\n",
    "            finalAnchorBoxesCoordinates = torch.Tensor([])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                finalScores = finalScores.cuda()\n",
    "                finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda()\n",
    "                finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda()\n",
    "\n",
    "            for i in range(classification.shape[2]):\n",
    "                scores = torch.squeeze(classification[:, :, i])\n",
    "                scores_over_thresh = (scores > 0.05)\n",
    "                if scores_over_thresh.sum() == 0:\n",
    "                    # no boxes to NMS, just continue\n",
    "                    continue\n",
    "\n",
    "                scores = scores[scores_over_thresh]\n",
    "                anchorBoxes = torch.squeeze(transformed_anchors)\n",
    "                anchorBoxes = anchorBoxes[scores_over_thresh]\n",
    "                anchors_nms_idx = nms(anchorBoxes, scores, 0.5)\n",
    "\n",
    "                finalResult[0].extend(scores[anchors_nms_idx])\n",
    "                finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
    "                finalResult[2].extend(anchorBoxes[anchors_nms_idx])\n",
    "\n",
    "                finalScores = torch.cat((finalScores, scores[anchors_nms_idx]))\n",
    "                finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0])\n",
    "                if torch.cuda.is_available():\n",
    "                    finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda()\n",
    "\n",
    "                finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue))\n",
    "                finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx]))\n",
    "\n",
    "            return [finalScores, finalAnchorBoxesIndexes, finalAnchorBoxesCoordinates]\n",
    "\n",
    "\n",
    "def resnet18(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "#* Map the model depths to their respective model variants\n",
    "resnet_fn_map = {\n",
    "    18: resnet18,\n",
    "    34: resnet34,\n",
    "    50: resnet50,\n",
    "    101: resnet101,\n",
    "    152: resnet152\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d95020",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe25343",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "This replaces the argprase in the script. Here, it is designed to be able to be easily configurable in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(5000) # set random seed for reproducibility\n",
    "\n",
    "CONFIG = {\n",
    "    'dataset': 'coco',  # Dataset type, must be one of csv or coco\n",
    "    'coco_path': '/path/to/your/coco/dataset',  # Path to COCO directory\n",
    "    'depth': 50,  # Resnet depth, must be one of 18, 34, 50, 101, 152\n",
    "    'epochs': 100,  # Number of epochs\n",
    "    'batch_size': 2,  # Training batch size\n",
    "    'learning_rate': 5e-4,  # Learning rate\n",
    "    'num_workers': 3,  # Number of data loader workers\n",
    "    'weights_save_dir': './saved_models/',  # Directory to save final model\n",
    "    'epoch_weights_dir': '/home/toni_intern/RetinaNet/epoch_weights/',  # Directory to save epoch weights\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d5273",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f32109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_retinanet(config):\n",
    "    \"\"\"\n",
    "    Train RetinaNet model with the given configuration.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing training parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    #* Create the data loaders\n",
    "    if config['dataset'] == 'coco':\n",
    "        if config['coco_path'] is None:\n",
    "            raise ValueError('Must provide coco_path when training on COCO')\n",
    "\n",
    "        dataset_train = CocoDataset(config['coco_path'], set_name='train',\n",
    "                                    transform=data_transform['train'])\n",
    "        dataset_val = CocoDataset(config['coco_path'], set_name='test',\n",
    "                                  transform=data_transform['test'])\n",
    "    else:\n",
    "        raise ValueError('Dataset type not understood (must be csv or coco), exiting.')\n",
    "\n",
    "    sampler = AspectRatioBasedSampler(dataset_train, batch_size=config['batch_size'], drop_last=False)\n",
    "    dataloader_train = DataLoader(dataset_train, num_workers=config['num_workers'], collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "        dataloader_val = DataLoader(dataset_val, num_workers=config['num_workers'], collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "    # Create the model\n",
    "    if config['depth'] == 18:\n",
    "        retinanet = resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif config['depth'] == 34:\n",
    "        retinanet = resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif config['depth'] == 50:\n",
    "        retinanet = resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif config['depth'] == 101:\n",
    "        retinanet = resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif config['depth'] == 152:\n",
    "        retinanet = resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "    else:\n",
    "        retinanet = torch.nn.DataParallel(retinanet)\n",
    "\n",
    "    retinanet.training = True\n",
    "\n",
    "    optimizer = optim.Adam(retinanet.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "    loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "    retinanet.train()\n",
    "    retinanet.module.freeze_bn()\n",
    "\n",
    "    print('Num training images: {}'.format(len(dataset_train)))\n",
    "\n",
    "    #* Training Phase\n",
    "    for epoch_num in range(config['epochs']):\n",
    "\n",
    "        retinanet.train()\n",
    "        retinanet.module.freeze_bn()\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for iter_num, data in enumerate(dataloader_train):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "                else:\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].float(), data['annot']])\n",
    "                    \n",
    "                classification_loss = classification_loss.mean()\n",
    "                regression_loss = regression_loss.mean()\n",
    "\n",
    "                loss = classification_loss + regression_loss\n",
    "\n",
    "                if bool(loss == 0):\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_hist.append(float(loss))\n",
    "\n",
    "                epoch_loss.append(float(loss))\n",
    "\n",
    "                print(\n",
    "                    'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "                        epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "                del classification_loss\n",
    "                del regression_loss\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # Validation phase\n",
    "        if dataset_val is not None:\n",
    "            print('Running validation...')\n",
    "            retinanet.eval()\n",
    "            val_loss = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_iter, val_data in enumerate(dataloader_val):\n",
    "                    if torch.cuda.is_available():\n",
    "                        val_classification_loss, val_regression_loss = retinanet([val_data['img'].cuda().float(), val_data['annot']])\n",
    "                    else:\n",
    "                        val_classification_loss, val_regression_loss = retinanet([val_data['img'].float(), val_data['annot']])\n",
    "                    \n",
    "                    val_classification_loss = val_classification_loss.mean()\n",
    "                    val_regression_loss = val_regression_loss.mean()\n",
    "                    val_total_loss = val_classification_loss + val_regression_loss\n",
    "                    \n",
    "                    val_loss.append(float(val_total_loss))\n",
    "            \n",
    "            avg_val_loss = np.mean(val_loss)\n",
    "            print(f'Epoch {epoch_num} - Training Loss: {np.mean(epoch_loss):.5f}, Validation Loss: {avg_val_loss:.5f}')\n",
    "            \n",
    "            # COCO evaluation (more comprehensive metrics)\n",
    "            if config['dataset'] == 'coco':\n",
    "                print('Evaluating COCO metrics...')\n",
    "                evaluate_coco(dataset_val, retinanet)\n",
    "\n",
    "        scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "        # Save epoch weights\n",
    "        os.makedirs(config['epoch_weights_dir'], exist_ok=True)\n",
    "        torch.save(retinanet.module, f\"{config['epoch_weights_dir']}{config['dataset']}_retinanet_depth_{config['depth']}_epoch_{epoch_num}.pt\")\n",
    "\n",
    "    retinanet.eval()\n",
    "\n",
    "    #* Save final model\n",
    "    os.makedirs(config['weights_save_dir'], exist_ok=True)\n",
    "    torch.save(retinanet, f\"{config['weights_save_dir']}depth_{config['depth']}_epochs_{config['epochs']}_model_final.pt\")\n",
    "    \n",
    "    print(f\"Training completed! Final model saved to: {config['weights_save_dir']}depth_{config['depth']}_epochs_{config['epochs']}_model_final.pt\")\n",
    "    \n",
    "    return retinanet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878d893",
   "metadata": {},
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d32589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Update the configuration as needed\n",
    "    CONFIG['coco_path'] = '/path/to/your/coco/dataset'  # Update this path\n",
    "    CONFIG['epochs'] = 10  # Reduce for quick testing\n",
    "    CONFIG['depth'] = 50   # Choose your ResNet depth\n",
    "    \n",
    "    # Start training\n",
    "    trained_model = train_retinanet(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cbf93",
   "metadata": {},
   "source": [
    "### What Can Be Customized in the Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fa9a0",
   "metadata": {},
   "source": [
    "1. **Optimizer** - These are not big factors that can improve the model, but in the case that the hyperparameters are not optimizing the model, changing the optimizer itself or the arguments may help improve its training.\n",
    "\n",
    "2. **Schedulers** - Schedulers can also improve the training by continuously chaing the optimizer's learning rate. This differs from the decay because this can reset the learnign rate to an optimal value at the end of an epoch, to start the next iteration.\n",
    "\n",
    "3. The configuration parameters\n",
    "\n",
    "4. The model, which should be another **single-stage object detector** (YOLO,Fast R-CNN, etc.). Transformers are trained with a different pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf61b4",
   "metadata": {},
   "source": [
    "## Loading Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc98f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
