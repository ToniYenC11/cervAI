{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfbcce",
   "metadata": {},
   "source": [
    "# RetinaNet for Object Detection\n",
    "\n",
    "This notebook documents the training done on `RetinaNet` for datasets sourced by cerv.AI team. The said datasets are as follows:\n",
    "1. https://universe.roboflow.com/cervitester-colposcopy-yihp4/colposcopy\n",
    "2. https://universe.roboflow.com/cervitester-colposcopy-yihp4/acetic_acid\n",
    "3. https://universe.roboflow.com/madhura/merged-acetic-acid/dataset/3\n",
    "\n",
    "## Additional Notes\n",
    "1. Due to the balanced nature of the datasets, this training provides **no data augmentations**. It is only when the training with the other datasets that augmentations will be applied.\n",
    "    - However, RetinaNet is excellent for imbalanced datasets due to the **Focal Loss** function.\n",
    "\n",
    "2. The datasets are all from **IARC Cervical Image Cancer Bank**\n",
    "\n",
    "## References \n",
    "1. [Fine-Tuning an Object Detection Model](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "2. [Focal Loss](https://paperswithcode.com/method/focal-loss)\n",
    "3. [RetinaNet (Theory)](https://paperswithcode.com/method/retinanet)\n",
    "4. [RetinaNet (Pytorch implementation)](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html)\n",
    "5. [Paper on Focal Loss](https://arxiv.org/abs/1708.02002)\n",
    "6. [Blog 1 on RetinaNet](https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d)\n",
    "7. [Blog 2 on RetinaNet](https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/)\n",
    "8. [Blog 3 on RetinaNet](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4)\n",
    "9. [Blog 4 on RetinaNet](https://analyticsindiamag.com/what-is-retinanet-ssd-focal-loss/)\n",
    "10. [Blog 5 on RetinaNet](https://towardsdatascience.com/object-detection-on-aerial-imagery-using-retinanet-626130ba2203)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa194c81",
   "metadata": {},
   "source": [
    "## Dataset Class and DataLoading\n",
    "\n",
    "The dataset class is based on the typical COCO format. Instead of storing the image on dedicated arrays, which results in large space complexity, the dataset instead accesses the image on the `load_image` method through the root folder, set name (either as train or test), and the image IDs.\n",
    "\n",
    "The dataloader stores the methods for each images. For each iteration of the pipline, the dataloader appleis the necessary methods to each dataset itself. You can refer to the training pipeline to see where this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import collections\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import skimage\n",
    "\n",
    "#* Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#* Dataset class\n",
    "class CocoDataset(Dataset):\n",
    "    \"\"\"Coco dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, set_name='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): COCO directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.set_name = set_name\n",
    "        self.transform = transform\n",
    "\n",
    "        self.coco      = COCO(os.path.join(self.root_dir, self.set_name,'_annotations.coco.json'))\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "\n",
    "        self.load_classes()\n",
    "\n",
    "    def load_classes(self):\n",
    "        # load class names (name -> label)\n",
    "        categories = self.coco.loadCats(self.coco.getCatIds())\n",
    "        categories.sort(key=lambda x: x['id'])\n",
    "\n",
    "        self.classes             = {}\n",
    "        self.coco_labels         = {}\n",
    "        self.coco_labels_inverse = {}\n",
    "        for c in categories:\n",
    "            self.coco_labels[len(self.classes)] = c['id']\n",
    "            self.coco_labels_inverse[c['id']] = len(self.classes)\n",
    "            self.classes[c['name']] = len(self.classes)\n",
    "\n",
    "        # also load the reverse (label -> name)\n",
    "        self.labels = {}\n",
    "        for key, value in self.classes.items():\n",
    "            self.labels[value] = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.load_image(idx)\n",
    "        annot = self.load_annotations(idx)\n",
    "        sample = {'img': img, 'annot': annot}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        path       = os.path.join(self.root_dir,self.set_name, image_info['file_name'])\n",
    "        img = skimage.io.imread(path)\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = skimage.color.gray2rgb(img)\n",
    "\n",
    "        return img.astype(np.float32)/255.0\n",
    "\n",
    "    def load_annotations(self, image_index):\n",
    "        # get ground truth annotations\n",
    "        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n",
    "        annotations     = np.zeros((0, 5))\n",
    "\n",
    "        # Catch function in case an image has no annotations\n",
    "        if len(annotations_ids) == 0:\n",
    "            return annotations\n",
    "\n",
    "        # parse annotations\n",
    "        coco_annotations = self.coco.loadAnns(annotations_ids)\n",
    "        for idx, a in enumerate(coco_annotations):\n",
    "\n",
    "            # some annotations have basically no width / height, skip them\n",
    "            if a['bbox'][2] < 1 or a['bbox'][3] < 1:\n",
    "                continue\n",
    "\n",
    "            annotation        = np.zeros((1, 5))\n",
    "            annotation[0, :4] = a['bbox']\n",
    "            annotation[0, 4]  = self.coco_label_to_label(a['category_id'])\n",
    "            annotations       = np.append(annotations, annotation, axis=0)\n",
    "\n",
    "        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n",
    "        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def coco_label_to_label(self, coco_label):\n",
    "        return self.coco_labels_inverse[coco_label]\n",
    "\n",
    "\n",
    "    def label_to_coco_label(self, label):\n",
    "        return self.coco_labels[label]\n",
    "\n",
    "    def image_aspect_ratio(self, image_index):\n",
    "        image = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        return float(image['width']) / float(image['height'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401bc1a",
   "metadata": {},
   "source": [
    "## Loading of Data to DataLoader\n",
    "\n",
    "The `DataLoader` contains a `collater` and `sampler`. \n",
    "\n",
    "- The `collater` merges a list of samples to form a mini-batch of Tensor(s), which is useful for batched loading.\n",
    "    - In simpler (but not exact) terms, it returns the images, annotations, and additional padding if defined into tensors\n",
    "    - These can be therefore, easily learned by the model and parallelized using batched nodes\n",
    "- The `sampler` defines the strategy to draw samples from the dataset. If specified, shuffle must not be specified.\n",
    "    - In some cases, ussing `shuffle` is simpler, but a dedicated `AspectRatioBasedSampler` function is contained\n",
    "    - It simply uses the `random` library's shuffle function. In reality, the simple `shuffle=True` also works.\n",
    "\n",
    "- Use the `transforms` library of torch to apply your own transformations in `data_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf86d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 dataloader workers every process\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "device = \"cuda\"\n",
    "root_dirs = ['datasets/merged']\n",
    "\n",
    "#* Custom collater for the dataloader\n",
    "def collater(data):\n",
    "\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [s['annot'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "        \n",
    "    widths = [int(s.shape[0]) for s in imgs]\n",
    "    heights = [int(s.shape[1]) for s in imgs]\n",
    "    batch_size = len(imgs)\n",
    "\n",
    "    max_width = np.array(widths).max()\n",
    "    max_height = np.array(heights).max()\n",
    "\n",
    "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = imgs[i]\n",
    "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "    \n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                #print(annot.shape)\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "\n",
    "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': padded_imgs, 'annot': annot_padded, 'scale': scales}\n",
    "\n",
    "#* sampling method\n",
    "class AspectRatioBasedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size, drop_last):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.groups = self.group_images()\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.groups)\n",
    "        for group in self.groups:\n",
    "            yield group\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.data_source) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def group_images(self):\n",
    "        # determine the order of the images\n",
    "        order = list(range(len(self.data_source)))\n",
    "        order.sort(key=lambda x: self.data_source.image_aspect_ratio(x))\n",
    "\n",
    "        # divide into groups, one group = one batch\n",
    "        return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]\n",
    "\n",
    "#* Data Transformation (You can define your own here)\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"test\": transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "dataset_train = CocoDataset(root_dir=\"/datasets\",set_name=\"train\",transform=data_transform['train']) \n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False)\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4585a9",
   "metadata": {},
   "source": [
    "## Loss Function and Evalutation Metrics\n",
    "\n",
    "## Focal Loss\n",
    "Formally, it is given by the following:\n",
    "$$FL_{p_t} = -(1-p)^{\\gamma}\\log(p_t)$$\n",
    "where $\\gamma$ is a tunable parameter. It basically adds a loss factor $-(1-p)^{\\gamma}$ to the standard cross entropy criterion. \n",
    "- Setting $\\gamma > 0$ reduces the relative loss for well-classified examples ($p_t > 5), emphasizing the hard, misclassified examples.\n",
    "\n",
    "## Evaluation Metrics\n",
    "### Intersection over Union (IoU)\n",
    "The evaluation metric defined in the [docs](https://pytorch.org/ignite/generated/ignite.metrics.IoU.html). Here however, it is hardcoded so as to make it more appropriate to the task at hand.\n",
    "\n",
    "## Some Classes and Functions\n",
    "- `gather` : Gathers values along an axis specified by *dim*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5204ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "# from torchvision.models import ResNet50_Weights\n",
    "from torchinfo import summary\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "#* Intersection over Union\n",
    "def calc_iou(a, b):\n",
    "    \"\"\"\n",
    "    Defines the Intersection over Union (IoU) metric which utilizes the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU\n",
    "\n",
    "\n",
    "#* Define the Focal Loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the Focal Loss criterion. See the documentation in the notebook.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2, alpha = 0.25, size_average = True): #For default values: https://pytorch.org/vision/main/generated/torchvision.ops.sigmoid_focal_loss.html\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha]) # store the p and 1-p\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "    \n",
    "    def forward(self,input,target):\n",
    "        if input.dim() > 2:\n",
    "            # Applying transmutations to data\n",
    "            input = input.view(input.size(0), input.size(1),-1) # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2) #N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2)) # N,H*W, C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "        \n",
    "        # Logits calculation for probability\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1) # Convert to row vec\n",
    "        pt = logpt.data.exp()\n",
    "\n",
    "        # if alpha is present\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data) #ensures the alpha is the same data type as input (Tensor)\n",
    "            at = self.alpha.gather()\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()\n",
    "        \n",
    "def evaluate_coco(dataset, model, threshold=0.05):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # start collecting results\n",
    "        results = []\n",
    "        image_ids = []\n",
    "\n",
    "        for index in range(len(dataset)):\n",
    "            image, target, scale = dataset[index]\n",
    "\n",
    "            # run network\n",
    "            if torch.cuda.is_available():\n",
    "                scores, labels, boxes = model(image.cuda().float().unsqueeze(0))\n",
    "            else:\n",
    "                scores, labels, boxes = model(image.float().unsqueeze(0))\n",
    "            scores = scores.cpu()\n",
    "            labels = labels.cpu()\n",
    "            boxes  = boxes.cpu()\n",
    "\n",
    "            # correct boxes for image scale\n",
    "            boxes /= scale\n",
    "\n",
    "            if boxes.shape[0] > 0:\n",
    "                # change to (x, y, w, h) (MS COCO standard)\n",
    "                boxes[:, 2] -= boxes[:, 0]\n",
    "                boxes[:, 3] -= boxes[:, 1]\n",
    "\n",
    "                # compute predicted labels and scores\n",
    "                #for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "                for box_id in range(boxes.shape[0]):\n",
    "                    score = float(scores[box_id])\n",
    "                    label = int(labels[box_id])\n",
    "                    box = boxes[box_id, :]\n",
    "\n",
    "                    # scores are sorted, so we can break\n",
    "                    if score < threshold:\n",
    "                        break\n",
    "\n",
    "                    # append detection for each positively labeled class\n",
    "                    image_result = {\n",
    "                        'image_id'    : dataset.image_ids[index],\n",
    "                        'category_id' : dataset.label_to_coco_label(label),\n",
    "                        'score'       : float(score),\n",
    "                        'bbox'        : box.tolist(),\n",
    "                    }\n",
    "\n",
    "                    # append detection to results\n",
    "                    results.append(image_result)\n",
    "\n",
    "            # append image to list of processed images\n",
    "            image_ids.append(dataset.image_ids[index])\n",
    "\n",
    "            # print progress\n",
    "            print('{}/{}'.format(index, len(dataset)), end='\\r')\n",
    "\n",
    "        if not len(results):\n",
    "            return\n",
    "\n",
    "        # write output\n",
    "        json.dump(results, open('{}_bbox_results.json'.format(dataset.set_name), 'w'), indent=4)\n",
    "\n",
    "        # load results in COCO evaluation tool\n",
    "        coco_true = dataset.coco\n",
    "        coco_pred = coco_true.loadRes('{}_bbox_results.json'.format(dataset.set_name))\n",
    "\n",
    "        # run COCO evaluation\n",
    "        coco_eval = COCOeval(coco_true, coco_pred, 'bbox')\n",
    "        coco_eval.params.imgIds = image_ids\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        model.train()\n",
    "        # Optional: extract mAP@0.5 (index 1) and mAP@0.5:0.95 (index 0)\n",
    "        map_50 = coco_eval.stats[1]\n",
    "        map_5095 = coco_eval.stats[0]\n",
    "        print(f\"mAP@0.5: {map_50:.4f}, mAP@0.5:0.95: {map_5095:.4f}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecae2a",
   "metadata": {},
   "source": [
    "## RetinaNet-RS\n",
    "\n",
    "See the documentation [here](https://paperswithcode.com/method/retinanet-rs). It also has a medium article [here](https://freedium.cfd/https://medium.com/@evertongomede/retinanet-advancing-object-detection-in-computer-vision-719ceb744308).\n",
    "\n",
    "- Class imbalance has impede training due to overfitting on dominant classes. This is due to traditional loss functions treating all classes equally.\n",
    "\n",
    "- **Focal loss** addresses this issue by dynamically down-weighing the contribution of well-classified examples while emphasizing the importance of hard-to-classify examples. \n",
    "- This is achieved by introducing a **modulating factor** that reduces the loss for well-classified examples and increases the loss for misclassified examples\n",
    "- For the classification head, a separate model is used as a backbone. To use more efficient resources, we will be utilizing the smaller model **ResNet50** with 50 neural layers.\n",
    "\n",
    "## Some Important Model Notes (see article [here](https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d))\n",
    "- **Anchor boxes** are used to generate the region proposals. Previously, selective search and edge boxes weere used. However, this was impossible to generate with standard convolutions that are utilized by most CNNs.\n",
    "\n",
    "- Each representative region (in the article, it is a 50x50 pixel) is fed to a **regression head** and a **classification head**. Usually used is the ResNet models.\n",
    "- However, the feature map created after multiple subsampling loses a lot of semantic information at low level, thus unable to detet small objects in images. To solve this, the model uses **Feature Pyramid Networks**\n",
    "\n",
    "### [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144)\n",
    "- Though convnets are robust to variance in scale, all the top entries in ImageNet or COCO have used multi-scale testing on featurized image pyramids.\n",
    "\n",
    "- We have to take images at different sizes say 256 x 256, 300 x 300, 500 x 500 and 800 x 800 etc, calculate feature maps for each of this image and then apply non-maxima supression over all these detected positive anchor boxes. This is a very costly operation and inference times gets high.\n",
    "\n",
    "- The authors of this paper observed that deep convnet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. For example, take a Resnet architecture and instead of just using the final feature map as shown in RPN network, take feature maps before every pooling (subsampling) layer. \n",
    "\n",
    "- Perform the same operations as for **Region Proposal Network** RPN on each of these feature maps and finally combine them using non-maxima supression. This is the crude way of building the feature pyramid networks. \n",
    "\n",
    "- But there are large semantic gaps caused by different depths. The high resolution maps (earlier layers) have low-level features that harm their representational capacity for object detection. To achieve this goal, the authors relayed on a architecture that combines low-resolution, semantically strong features with high-resolution, semantically strong features via top-down pathway and lateral connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0489f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "from retinanet.utils import BasicBlock, Bottleneck, BBoxTransform, ClipBoxes\n",
    "from retinanet.anchors import Anchors\n",
    "\n",
    "#* Store the urls of all ResNet models to easily swap to other models\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "#* Define the RetinaNet Model architecture: Regression Head, Pyramid Networks, Classification Backbone, etc.\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
    "        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
    "        self.P7_1 = nn.ReLU()\n",
    "        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        C3, C4, C5 = inputs\n",
    "\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "\n",
    "        P4_x = self.P4_1(C4)\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "\n",
    "        P3_x = self.P3_1(C3)\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "\n",
    "        P6_x = self.P6(C5)\n",
    "\n",
    "        P7_x = self.P7_1(P6_x)\n",
    "        P7_x = self.P7_2(P7_x)\n",
    "\n",
    "        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        # out is Batch_size x Channels x Width x Height, with C = 4*num_anchors\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        # After self.output, the shape is [B, C, W, H].\n",
    "        # The model permutes it to [B, W, H, C] so that:\n",
    "        # For every pixel location on the feature map (W x H), it has all anchor predictions grouped together.\n",
    "        # Easier to reshape into a flat list of bounding box predictions.\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 4)\n",
    "        \n",
    "        #After permute: shape = [B, W, H, num_anchors * 4]\n",
    "        #.view(B, -1, 4) collapses spatial dimensions and anchors into a single list of box predictions:\n",
    "        #Output shape: [B,W×H×num_anchors,4]\n",
    "        #Output shape: [B,W×H×num_anchors,4]\n",
    "\n",
    "        #So you're getting a 3D tensor where:\n",
    "        #- B is batch size.\n",
    "        #- Second dim is the total number of anchor boxes.\n",
    "        #- Last dim is the 4 regression values per anchor.\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.act4(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "        out = self.output_act(out)\n",
    "\n",
    "        # out is B x C x W x H, with C = n_classes + n_anchors\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, channels = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, block, layers):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        if block == BasicBlock:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv2.out_channels, self.layer3[layers[2] - 1].conv2.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv2.out_channels]\n",
    "        elif block == Bottleneck:\n",
    "            fpn_sizes = [self.layer2[layers[1] - 1].conv3.out_channels, self.layer3[layers[2] - 1].conv3.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv3.out_channels]\n",
    "        else:\n",
    "            raise ValueError(f\"Block type {block} not understood\")\n",
    "\n",
    "        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2])\n",
    "\n",
    "        self.regressionModel = RegressionModel(256)\n",
    "        self.classificationModel = ClassificationModel(256, num_classes=num_classes)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "\n",
    "        self.focalLoss = FocalLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        prior = 0.01\n",
    "\n",
    "        self.classificationModel.output.weight.data.fill_(0)\n",
    "        self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
    "\n",
    "        self.regressionModel.output.weight.data.fill_(0)\n",
    "        self.regressionModel.output.bias.data.fill_(0)\n",
    "\n",
    "        self.freeze_bn()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if self.training:\n",
    "            img_batch, annotations = inputs\n",
    "        else:\n",
    "            img_batch = inputs\n",
    "\n",
    "        x = self.conv1(img_batch)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        features = self.fpn([x2, x3, x4])\n",
    "\n",
    "        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n",
    "\n",
    "        anchors = self.anchors(img_batch)\n",
    "\n",
    "        if self.training:\n",
    "            return self.focalLoss(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
    "\n",
    "            finalResult = [[], [], []]\n",
    "\n",
    "            finalScores = torch.Tensor([])\n",
    "            finalAnchorBoxesIndexes = torch.Tensor([]).long()\n",
    "            finalAnchorBoxesCoordinates = torch.Tensor([])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                finalScores = finalScores.cuda()\n",
    "                finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda()\n",
    "                finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda()\n",
    "\n",
    "            for i in range(classification.shape[2]):\n",
    "                scores = torch.squeeze(classification[:, :, i])\n",
    "                scores_over_thresh = (scores > 0.05)\n",
    "                if scores_over_thresh.sum() == 0:\n",
    "                    # no boxes to NMS, just continue\n",
    "                    continue\n",
    "\n",
    "                scores = scores[scores_over_thresh]\n",
    "                anchorBoxes = torch.squeeze(transformed_anchors)\n",
    "                anchorBoxes = anchorBoxes[scores_over_thresh]\n",
    "                anchors_nms_idx = nms(anchorBoxes, scores, 0.5)\n",
    "\n",
    "                finalResult[0].extend(scores[anchors_nms_idx])\n",
    "                finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
    "                finalResult[2].extend(anchorBoxes[anchors_nms_idx])\n",
    "\n",
    "                finalScores = torch.cat((finalScores, scores[anchors_nms_idx]))\n",
    "                finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0])\n",
    "                if torch.cuda.is_available():\n",
    "                    finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda()\n",
    "\n",
    "                finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue))\n",
    "                finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx]))\n",
    "\n",
    "            return [finalScores, finalAnchorBoxesIndexes, finalAnchorBoxesCoordinates]\n",
    "\n",
    "\n",
    "def resnet18(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(num_classes, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(num_classes, Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152'], model_dir='.'), strict=False)\n",
    "    return model\n",
    "\n",
    "#* Map the model depths to their respective model variants\n",
    "resnet_fn_map = {\n",
    "    18: resnet18,\n",
    "    34: resnet34,\n",
    "    50: resnet50,\n",
    "    101: resnet101,\n",
    "    152: resnet152\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d95020",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f32109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "list indices must be integers or slices, not str\n",
      "Evaluating dataset\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'COCODataset' object has no attribute 'image_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEvaluating dataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mevaluate_coco\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretinanet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m scheduler.step(np.mean(epoch_loss))\n\u001b[32m     70\u001b[39m torch.save(retinanet.module, \u001b[33m'\u001b[39m\u001b[33mweights/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_retinanet_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m.format(retinanet, num_epochs))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mevaluate_coco\u001b[39m\u001b[34m(dataset, model, threshold)\u001b[39m\n\u001b[32m    123\u001b[39m         results.append(image_result)\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# append image to list of processed images\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m image_ids.append(\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_ids\u001b[49m[index])\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# print progress\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(index, \u001b[38;5;28mlen\u001b[39m(dataset)), end=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'COCODataset' object has no attribute 'image_ids'"
     ]
    }
   ],
   "source": [
    "depth = 18\n",
    "if depth not in resnet_fn_map:\n",
    "        raise ValueError(f\"Unsupported ResNet depth: {depth}. Choose from {list(resnet_fn_map.keys())}.\")\n",
    "retinanet = resnet_fn_map[depth](num_classes=2,pretrained=True)\n",
    "\n",
    "#* Optimizers and schedulers\n",
    "optimizer = torch.optim.Adam(retinanet.parameters(),lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "loss = FocalLoss()\n",
    "loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "#TODO: Create pipeline for hyperparametertuning of model and optimizers\n",
    "\n",
    "# Use CUDA GPU\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    if torch.cuda.is_available():\n",
    "        retinanet = retinanet.cuda()\n",
    "if torch.cuda.is_available():\n",
    "    retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "else:\n",
    "    retinanet = torch.nn.DataParallel(retinanet)\n",
    "\n",
    "#* Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    retinanet.train()\n",
    "    retinanet.module.freeze_bn()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for iter_num, data in enumerate(train_loader):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "            else:\n",
    "                classification_loss, regression_loss = retinanet([data['img'].float(), data['annot']])\n",
    "                \n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            if bool(loss == 0):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "            print(\n",
    "                'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "                    epoch, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    print('Evaluating dataset')\n",
    "    evaluate_coco(test_dataset, retinanet)\n",
    "    scheduler.step(np.mean(epoch_loss))\n",
    "    torch.save(retinanet.module, 'weights/{}_retinanet_{}.pt'.format(retinanet, num_epochs))\n",
    "retinanet.eval()\n",
    "torch.save(retinanet,f'model_resnet{depth}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa65d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n",
      "<class 'tuple'> 2\n"
     ]
    }
   ],
   "source": [
    "def test_coco(dataset, model, threshold=0.05):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index in range(len(dataset)):\n",
    "            data = dataset[index]\n",
    "            print(type(data), len(data))\n",
    "            \n",
    "test_coco(test_dataset,retinanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc98f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
